{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d214d448",
   "metadata": {},
   "source": [
    "*This notebook is part of  course materials for CS 345: Machine Learning Foundations and Practice at Colorado State University.\n",
    "Original versions were created by Asa Ben-Hur and updated by Ross Beveridge.\n",
    "The content is availabe [on GitHub](https://github.com/asabenhur/CS345).*\n",
    "\n",
    "*The text is released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/), and code is released under the [MIT license](https://opensource.org/licenses/MIT).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca6ee0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github//asabenhur/CS345/blob/master/fall22/notebooks/module05_01_hyperparameters_validation.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b25e7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddc7d1",
   "metadata": {},
   "source": [
    "# Hyper-parameter selection using a validation set\n",
    "\n",
    "### Classifier parameters vs hyper-parameters\n",
    "\n",
    "Training of a classifier consists of using the training data to find good values for the *parameters* of the model (e.g. the weight vector of the perceptron algorithm).\n",
    "All the classifiers we have seen thus far have additional parameters that control the classifier's training, and need to set by the user.  These are called *hyper-parameters*. For example:\n",
    "\n",
    "* KNN:  number of nearest neighbors\n",
    "* Perceptron:  learning rate\n",
    "* Ridge regression:  regularization parameter\n",
    "* Non-linear SVM:  soft-margin constant and kernel parameter (degree of polynomial kernel or width of Gaussian kernel).\n",
    "\n",
    "\n",
    "### The wrong way to select hyper-parameters\n",
    "\n",
    "Our temptation is to do the following:\n",
    "\n",
    "* Divide the data into train and test sets\n",
    "* Loop over a set of potential values for the hyper-parameter(s)\n",
    "* For each value of the hyper-parmeter(s) train a classifier on the training set and evaluate it on the test set\n",
    "* Report to the user the value of the best performing classifier\n",
    "\n",
    "Why is this wrong?  The choice of which value to report to the user uses information about the test set, and the end-result is an accuracy estimate that is over-optimistic.  *Your estimates of classifier performance should never be based on performance on the test set*!\n",
    "So what can we do?\n",
    "\n",
    "### Use a validation set!\n",
    "\n",
    "Here's a variation of the above procedure, that introduces the idea of using a **validation set**, a subset of the data used for choosing hyperparameters.\n",
    "\n",
    "* Divide the data into **training, validation, and test** sets\n",
    "* Loop over a set of potential values for the hyper-parameter(s)\n",
    "* For each value of the hyper-parmeter(s) train a classifier on the training set and evaluate it on the **validation** set\n",
    "* Choose the best performing classifier based on its performance on the validation set, and evaluate its performance on the test set.\n",
    "* Report to the user the test-set performance of the classifier chosen based on its performance on the validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed6221",
   "metadata": {},
   "source": [
    "Let's apply this to the breast cancer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d221c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X,y = data = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfdd819",
   "metadata": {},
   "source": [
    "Our next step is to split the data into training/validation/test set. \n",
    "scikit-learn does not have a method for a three-way split of a dataset, so we'll use `train_test_split` twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "393feda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# first split into training / test, where the training set\n",
    "# will be further split into training / validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "# now split the initial training set into the final training\n",
    "# and validation sets:\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a632af",
   "metadata": {},
   "source": [
    "Our next step is to loop over a set of hyper-parameter values, and select the best performing value based on its performance on the validation set. We will do this for an SVM, and for simplicity, we will focus on choosing the value of a single parameter, the Gaussian width parameter, $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dbf733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity we will only consider the gamma hyper parameter:\n",
    "\n",
    "def svm_select_gamma(X_train, X_valid, y_train, y_valid,\n",
    "                     gammas) :\n",
    "    accuracies = []\n",
    "    for gamma in gammas : \n",
    "        classifier = svm.SVC(kernel=\"rbf\", gamma=gamma, C=10)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_valid)\n",
    "        accuracy = np.mean(y_valid == y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"gamma: {gamma}\\t accuracy: {accuracy:0.3f}\")\n",
    "    return gammas[np.argmax(accuracies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6311e4",
   "metadata": {},
   "source": [
    "The final step is to run this function and evaluate the resulting classifier over the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0144f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 1e-05\t accuracy: 0.758\n",
      "gamma: 0.0001\t accuracy: 0.956\n",
      "gamma: 0.001\t accuracy: 0.967\n",
      "gamma: 0.01\t accuracy: 0.978\n",
      "gamma: 0.1\t accuracy: 0.967\n",
      "gamma: 1.0\t accuracy: 0.670\n",
      "gamma: 10.0\t accuracy: 0.670\n",
      "gamma: 100.0\t accuracy: 0.670\n",
      "chosen value of gamma: \n",
      "accuracy on test set:  0.965\n"
     ]
    }
   ],
   "source": [
    "gammas = np.logspace(-5, 2, num=8, endpoint=True, base=10.0)\n",
    "gamma = svm_select_gamma(X_train, X_valid, y_train, y_valid, gammas)\n",
    "print(\"chosen value of gamma: \")\n",
    "classifier = svm.SVC(kernel=\"rbf\", gamma=gamma, C=10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(f'accuracy on test set:  {np.mean(y_test == y_pred):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a193d2c",
   "metadata": {},
   "source": [
    "### Final comments\n",
    "\n",
    "* When a classifier has multiple hyperparameters (e.g. non-linear SVM with soft-margin constant and kernel parameter) you would ideally run grid-search.\n",
    "* When you have a small amount of data, dividing it into training, validation, and test sets will leave you with datasets that are too small for effective training and evaluation.  In the next notebooks we will explore ways of addressing this issue and make more effective use of the data you have."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
