{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is part of  course materials for CS 345: Machine Learning Foundations and Practice at Colorado State University.\n",
    "Original versions were created by Asa Ben-Hur.\n",
    "The content is availabe [on GitHub](https://github.com/asabenhur/CS345).*\n",
    "\n",
    "*The text is released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/), and code is released under the [MIT license](https://opensource.org/licenses/MIT).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github//asabenhur/CS345/blob/master/fall22/notebooks/module05_03_model_selection.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection using cross-validation\n",
    "\n",
    "This notebook covers the topic of selecting classifier hyper-parameters using cross-validation.\n",
    "\n",
    "As discussed in an earlier notebook, whenever a classifier has hyperparameters that require setting, the appropriate protocol for evaluating classifier accuracy is as follows:\n",
    "\n",
    "* Divide the data into separate training, validation, and test sets\n",
    "* Loop over all combinations of values of hyperparameters\n",
    "* For each value combination, train a model on the training set and evaluate it on the validation set\n",
    "* Choose best performing set of parameters and retrain the classifier using the training and validation set.\n",
    "* Evaluate the resulting classifier on the test set\n",
    "\n",
    "Now that we know how to perform cross-validation, we can update this protocol to make use of this technique for more efficient use of our data.\n",
    "\n",
    "### Model selection using grid search in scikit-learn\n",
    "\n",
    "Before we describe the procedure for model selection using cross-validation, we will introduce scikit-learn functionality to perform model selection using grid-search, which will simplify the code for performing this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# you will get better results if you standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do plain cross-validation, without trying to choose a particular value for the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9753920198726906"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.SVC(kernel=\"rbf\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "accuracy = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This used the default parameter values chosen by the developers of scikit-learn.  The defaults are (usually) quite good, but each classifier has parameters that require tuning for optimal performance on a given problem.  **Typically there is no parameter choice that would be universally good** (among those parameters that have a strong effect on classifier performance).\n",
    "So let's see if we can do better if we choose parameters optimized for the given dataset rather than using the defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# the following defines two grids - one for the linear kernel\n",
    "# and a second grid for the Gaussian kernel\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], \n",
    "   'gamma': [0.001, 0.01, 0.1], \n",
    "   'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# instantiate a GridSearchCV object with SVM as the underlying classifier\n",
    "# with the grid defined above\n",
    "classifier = GridSearchCV(svm.SVC(), param_grid)\n",
    "\n",
    "classifier.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# for the nearest neighbor classifier we can select multiple \n",
    "# hyperparameters:\n",
    "# n_neighbors - number of neighbors to base classification on\n",
    "# p - whether to use the L1 or L2 norm when computing distances\n",
    "# weights - the sklearn KNN implementation provides the option to\n",
    "# weight the contribution of each example uniformly or inversely \n",
    "# proportional to its distance\n",
    "# this leads to the following grid of values:\n",
    "\n",
    "param_grid = {'n_neighbors': [3,5,7,9,11,13],\n",
    "              'p': [1, 2],\n",
    "              'weights' : ['uniform', 'distance']}\n",
    "\n",
    "classifier = GridSearchCV(KNeighborsClassifier(), param_grid)\n",
    "\n",
    "classifier.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you `fit` a `GridSearchCV` runs the following grid-search procedure:\n",
    "\n",
    "**Training using grid-search**:\n",
    "\n",
    "**Input**:  a classifier and a set of parameters with a set of values to try for each parameter.\n",
    "\n",
    "* Loop over all combinations of classifier parameters.\n",
    "* For each value of the classifier parameters, perform cross-validation on the training data and evaluate accuracy\n",
    "* Use the best scoring model parameters, and retrain on the entire dataset \n",
    "\n",
    "In the example above for the nonlinear SVM parameter grid, grid search over the parameters `C` and `gamma` can be implemented using nested for loops for creating all combinations of values:\n",
    "\n",
    "```python\n",
    "for C in soft_margin_constant_list :\n",
    "    for gamma in gamma_list :\n",
    "        # evaluate SVM accuracy with cross-validation \n",
    "        # using C and n_neighbors while \n",
    "        # if accuracy is better than current best, keep \n",
    "        # those values\n",
    "```\n",
    "\n",
    "However, keep in mind that you are not limited by the number of parameters you can perform model selection over!\n",
    "\n",
    "The trained `GridSearchCV` object contains information regarding the best parameter values and detailed cross-validation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_C', 'param_kernel', 'param_gamma', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])\n",
      "[0.97014439 0.96665114 0.95964912 0.96138798 0.94730632 0.96663562\n",
      " 0.95958702 0.97365316 0.97893184 0.94725974 0.97014439 0.96837448\n",
      " 0.94902965 0.9719143  0.9648657  0.94902965]\n"
     ]
    }
   ],
   "source": [
    "print(classifier.cv_results_.keys())\n",
    "print(classifier.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ``fit`` method of the ``GridSearchCV`` classifier performed cross-validation to find the best parameters.  \n",
    "To find the performance of the classifier let's run cross-validation over our ``GridSearchCV`` classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789007918025151"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = cross_val_score(classifier, X, y, cv=cv, \n",
    "                           scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performed **nested cross-validation**:  for each fold of cross-validation, the classifier perform cross-validation to find the best parameters.\n",
    "\n",
    "<img style=\"padding: 10px; float:center;\" alt=\"nested cross validation; created by Asa Ben-Hur inspired by figure from https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch06/ch06.ipynb\" src=\"https://github.com/asabenhur/CS345/raw/master/fall20/notebooks/figures/nested_cross_validation.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross-validation vs train/validation/test \n",
    "\n",
    "Our original procedure for model which uses train/validation/test set splits is shown here:\n",
    "\n",
    "* Divide the data into separate train, validation, and test sets\n",
    "* Loop over all combinations of values of hyperparameters\n",
    "* For each value combination, train a model on the training set and evaluate it on the validation set\n",
    "* Choose best performing set of parameters and retrain the classifier using the training and validation set.\n",
    "* Evaluate the resulting classifier on the test set\n",
    "\n",
    "The cross-validation version is as follows:\n",
    "\n",
    "* Divide the data into $k$ subsets\n",
    "* Loop over the $k$ subset, and at each iteration one of them serves as a test set.\n",
    "  * Loop over all combinations of values of the hyperparameters\n",
    "  * For each value combination, evaluate the classifier using cross-validation\n",
    "  * Choose best performing set of parameters and re-train the classifier using all the training data for that fold.\n",
    "  * Evaluate the resulting classifier on the test set for the given fold\n",
    "* Combine accuracy estimates over all the folds.\n",
    "\n",
    "This process looks quite expensive.  Let's be more concrete and evaluate how many models we need to train when considering say 100 combinations of hyperparameter values and performing 5-fold cross-validation in both the inner and outer loops.\n",
    "For fitting the grid-search object: for each combination of hyperparameter values we run 5-fold cross-validation, resulting in training 500 models.\n",
    "If we are doing nested cross validation the overall number of models is $100 \\times 5 \\times 5 = 2,500$.\n",
    "The process of cross-validation and nested cross-validation is highly parallelizable since folds can be trained and evaluated independently of each other for each combination of values of the hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "\n",
    "\n",
    "* Perform model selection for [k-nearest neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and select the optimal value of the number of neighbors and whether to use Euclidean or Minkowski distances (the parameter `p` in scikit-learn's implementation).  For the number of neighbors parameter,  choose a wide enough range of values that makes sense.  Accuracy comparison should be performed using cross-validation.  What optimal values of the parameters were chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "If a full grid search is too expensive, you can choose to perform randomized grid search, which randomly chooses points from the grid of values.  This is implemented in scikit-learn as the class [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV).  In neural networks, which have a very large number of parameters a complete grid search is not feasible, and randomized grid search is the only viable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining unbiased results by preventing information leak between train and test data: an example using feature selection\n",
    "\n",
    "When selecting one or two hyperparameters, the bias in accuracy when comparing correct experimental protocol with a protocol that allows for information leakage between the train and test sets can be quite small.\n",
    "However, in situations where a large number of hyper-parameters needs to be selected the difference can be dramatic.  Consider as an example the problem of feature selection.  In this problem we can think of each feature as being associated with a hyper-parameter that controls whether that feature should be included.\n",
    "\n",
    "We will demonstrate this using a dataset describing the activity of a large number of genes, from which disease state can be inferred.  The particular dataset we will analyze looks at biological samples taken from leukemia patients with two types of leukemia: acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).  The data was taken from the following publication:\n",
    "\n",
    "* Golub, Todd R., et al. \"Molecular classification of cancer: class discovery and class prediction by gene expression monitoring.\" Science  (1999): 531-537."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72, 7128), (72,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "link = \"https://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv\"\n",
    "# retrieve the contents of the file\n",
    "contents = requests.get(link)\n",
    "lines = contents.text.split()\n",
    "# the data is in csv format and the labels appear in the first \n",
    "# row of the dataset\n",
    "class_convert = {'ALL':1, 'AML':0}\n",
    "y = np.array([class_convert[token] for token in lines[0].split(',')])\n",
    "X = np.array([ [float(token) for token in line.split(',')] \n",
    "              for line in lines[1:] ])\n",
    "X = X.transpose()\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's evaluate the accuracy of the scikit-learn perceptron using all the features (note - this is different than the version of the perceptron studied in class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9723809523809525"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = svm.SVC(kernel='linear')\n",
    "cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "accuracy = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll perform feature selection on the entire dataset - even though it's the wrong way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# feature selection using RFE\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "selector = RFE(svm.SVC(kernel='linear'), step=0.1, \n",
    "               n_features_to_select=20)\n",
    "\n",
    "# perform feature selection on the data\n",
    "X_sel = selector.fit_transform(X, y)\n",
    "print ('number of features: ', X_sel.shape[1])\n",
    "\n",
    "accuracy = cross_val_score(p, X_sel, y, cv=cv, scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed feature selection using a method called **Recursive Feature Elimination** (RFE).  RFE builds on the intuition that the magnitude of the weight vector of a linear classifier is a good indication of a feature's usefulness.\n",
    "Rather than simply removing the features with the lowest value of the weight vector, RFE alternates between training a linear classifier and removing a small number of features with the lowest weight vector magnitude.  RFE was proposed in the following publication:\n",
    "\n",
    "> Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., “Gene selection for cancer classification using support vector machines”, Mach. Learn., 46(1-3), 389–422, 2002.\n",
    "\n",
    "As proposed, it uses support vector machines as the base classifier, but it can be used with any linear classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides us with functionality for easily performing feature selection the correct way, which leads to a very different estimate of accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9304761904761906"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = RFE(svm.SVC(kernel='linear'), step=0.1, \n",
    "               n_features_to_select=20)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [('feature_selector', selector),\n",
    "     ('classifier', svm.SVC(kernel='linear'))])\n",
    "accuracy = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of using 20 features was arbitrary and might not be optimal so we can perform grid search over the number of selected features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9314285714285715"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, \n",
    "                          {'feature_selector__n_features_to_select' : \n",
    "                           [5, 10, 20, 30, 40, 50]})\n",
    "accuracy = cross_val_score(grid_search, X, y, cv=cv, scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing model selection over an attribute of a component of a pipeline requires you to access the name of the attribute in combination with the name your provided for that step in the pipeline.\n",
    "\n",
    "To determine the optimal number of features, use `fit` instead of cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_selector__n_features_to_select': 40}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X, y)\n",
    "grid_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
