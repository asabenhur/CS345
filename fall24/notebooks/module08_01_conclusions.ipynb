{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is part of  course materials for CS 345: Machine Learning Foundations and Practice at Colorado State University.\n",
    "Original versions were created by Asa Ben-Hur.\n",
    "The content is availabe [on GitHub](https://github.com/asabenhur/CS345).*\n",
    "\n",
    "*The text is released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/), and code is released under the [MIT license](https://opensource.org/licenses/MIT).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github//asabenhur/CS345/blob/master/fall22/notebooks/module08_01_conclusions.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Summary / Conclusions\n",
    "\n",
    "In this course we focused about what happens after we have a set of features that characterize our dataset.  However, there is a lot that a machine learning practitioner needs to consider before that:\n",
    "\n",
    "* What is the problem that you are trying to solve?  What kind of machine learning problem is it, and is there data that can be used to address it?\n",
    "\n",
    "* Do you have legal access to the data?\n",
    "\n",
    "* Do you understand the data?  Without understanding the data, you cannot design good features.  Your classifier is only as good as the features you provide it.  The saying \"garbage-in garbage-out\" definitely applies!\n",
    "\n",
    "* Is your data clean?\n",
    "\n",
    "\n",
    "Once you have a dataset it is time to consider which machine learning approach to use.  There are a lot of options, and it can be difficult to choose.  \n",
    "The following image from the scikit-learn webpage summarizes and provides a rough roadmap for making these choices:\n",
    "\n",
    "<img style=\"padding: 10px; float:center;\" alt=\"https://scikit-learn.org/stable/tutorial/machine_learning_map/\" src=\"https://scikit-learn.org/stable/_static/ml_map.png\n",
    "\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure is a rough description of the process of coming up with a machine learning model:\n",
    "\n",
    "<img style=\"padding: 10px; float:center;\" src=\"https://github.com/asabenhur/CS345/raw/master/fall20/notebooks/figures/machine_learning_process.svg\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following table can help you in that as well:  it compares the characteristics of the classification algorithms we studied.  Several of these algorithms can be applied to regression problems as well, so this applies there as well.\n",
    "\n",
    "| Algorithm /<br/>Characteristic |  logistic<br/> regression |  KNN  | decision<br/>trees  | ensemble<br/>methods |  feed-forward<br/>networks | SVM |\n",
    "|:-----------|:-------:|:--------:|:-------:|:------:|:-------:|:------:|\n",
    "| **Predictive power** | ✔️ | ✔️ | ❌ | ✔️✔️ | ✔️✔️ | ✔️✔️ |\n",
    "| **Interpretability** | ✔️ | ✔️ | ✔️ | ❌ | ❌ | ❌ |\n",
    "| **Scalability<br/>(large N)** | ✔️✔️ | ❌ | ✔️✔️ | ✔️✔️ | ❌ | ✔️✔️ |\n",
    "| **Scalability<br/>(large d)** | ✔️ | ❌ | ❌ | ✔️✔️ | ✔️ | ✔️✔️ |\n",
    "| **Handle categorical<br/>data** | ❌ | ❌ | ✔️✔️ | ✔️✔️ | ❌ | ❌ |\n",
    "| **Handle missing<br/>data** | ❌ | ❌ | ✔️✔️ | ✔️✔️ | ❌ | ❌ |\n",
    "\n",
    "Legend:  ✔️✔️: high performer,✔️: mid performer, ❌: low performer\n",
    "\n",
    "A few comments regarding this table:\n",
    "\n",
    "* This assessment is based on information gleaned from the literature and the author's experience.\n",
    "\n",
    "* Ensemble methods refer to random forests and the various flavors of gradient boosting.\n",
    "\n",
    "* Predictive power is a method's ability to produce \"state-of-the-art\" performance.  Note that neural networks have the potential for high accuracy, but this requires very careful model selection.  SVMs also require carefuly setting of hyperparameters, but the space of hyperparameters is much smaller and it is easier to find optimal hyperparameters.  Among the state-of-the-art methods, ensemble methods require the least amount of \"fiddling\", and often produce great results out of the box (as long as you use enough classifiers in your ensemble).\n",
    "\n",
    "* Scalability with respect to the number of examples ($N$) refers to the algorithm's ability to handle large datasets.  Note that SVMs are highly scalable for the linear version, but a lot less so for the nonlinear version.\n",
    "\n",
    "* Scalability with respect to the number of features ($d$) refers to the algorithm's robustness with respect to the presence of a large number of features, many of which might be irrelevant.\n",
    "\n",
    "* When it comes to neural networks, the table refers to feed forward networks (multi-layer perceptrons).  Deep learning is addressed below.\n",
    "\n",
    "* SVMs have another advantage over the listed classifiers in their flexibility to model a variety of data through their use of kernels.  Similar flexibility is afforded by deep learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about deep learning?\n",
    "\n",
    "In the above table we focused on standard feed-forward neural networks.  Now we all heard of deep learning.  What is the role of these methods in our machine learning toolbox?  Well, if you have image or text data you should definitely look into deep learning.  This also extends to problems where you have non-standard data like sequences (e.g. accoustic signals or DNA/protein sequences) and graphs.  The amazing flexibility and sophisticated architectures found in deep learning are definitely worth exploring in such cases.  However, keep in mind that they require lots of data for effective training.  \n",
    "\n",
    "On the flip side, if you have data like is fixed dimensional vectors like we have analyzed in this course, deep learning is not likely to help you much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you have learned\n",
    "\n",
    "In this course we covered a lot of ground.  You have gained an understanding of most of the steps that go into designing effective machine learning solutions:\n",
    "\n",
    "* Write effective and efficient code for manipulating matrix and vector data\n",
    "* Choosing the right way to formulate the problem\n",
    "* Accuracy depends on the quality of the features and how you choose to represent them (feature scaling/normalization)\n",
    "* Training vs testing accuracy\n",
    "* Overfitting\n",
    "* Model selection: the validation set\n",
    "* Beware of biases\n",
    "* Conducting meaningful machine learning experiments using scikit-learn\n",
    "* Present code, data, and results using Jupyter notebooks.\n",
    "\n",
    "### Topics we haven't covered\n",
    "\n",
    "Machine learning is a very broad area that is growing rapidly.  Here are some important aspects that were not covered:\n",
    "\n",
    "* Unsupervised learning:  dimensionality reduction (PCA/t-SNE) and clustering\n",
    "* Deep learning\n",
    "* Kernel methods and support vector machines\n",
    "* Graphical models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
