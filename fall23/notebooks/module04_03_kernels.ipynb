{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d214d448",
   "metadata": {},
   "source": [
    "*This notebook is part of  course materials for CS 345: Machine Learning Foundations and Practice at Colorado State University.\n",
    "Original versions were created by Asa Ben-Hur and updated by Ross Beveridge.\n",
    "The content is availabe [on GitHub](https://github.com/asabenhur/CS345).*\n",
    "\n",
    "*The text is released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/), and code is released under the [MIT license](https://opensource.org/licenses/MIT).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca6ee0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github//asabenhur/CS345/blob/master/fall23/notebooks/module04_03_kernels.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25e7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddc7d1",
   "metadata": {},
   "source": [
    "# The nonlinear SVM:  kernels\n",
    "\n",
    "In this notebook we will introduce the ability of SVM to generate non-linear decision boundaries by using the concept of a kernel.\n",
    "But first we will do so using the same trick we have previously used of mapping our data using a nonlinear function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3817b59",
   "metadata": {},
   "source": [
    "### Nonlinear decision boundaries using polynomial features\n",
    "\n",
    "We will show how to use polynomial features to handle data that is not linearly separable.\n",
    "\n",
    "As a toy example, we will use data generated using scikit-learn's [make_circles](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9edfda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X,y = make_circles(n_samples=200, noise=0.1, random_state=42, factor=0.5)\n",
    "# n_samples is the number of examples to generate\n",
    "# noise parameter is the standard deviation of Gaussian noise \n",
    "# added to the data.\n",
    "# factor: scale factor between inner and outer circle\n",
    "\n",
    "# next:  scale data to be between -1 and 1:\n",
    "\n",
    "X[:,0] = 2 * (X[:,0] - min(X[:,0])) / (max(X[:,0]) - min(X[:,0])) - 1\n",
    "X[:,1] = 2 * (X[:,1] - min(X[:,1])) / (max(X[:,1]) - min(X[:,1])) - 1\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax  = fig.add_subplot(111)\n",
    "colors = ['r' if y[i] == 0 else 'b' for i in range(len(y))]\n",
    "ax.scatter(X[:, 0], X[:, 1], c=colors, s=50, alpha=0.6) \n",
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04149763",
   "metadata": {},
   "source": [
    "Clearly, this data is not linearly separable.  To obtain a classifier that is able to classify this data we will use the same approach we used in basis function regression:  apply a nonlinear function to our data.  In this setup, rather than classify the original data, we will first apply a function $\\Phi(\\mathbf{x})$ to our feature vectors:\n",
    "\n",
    "$$\n",
    "\\Phi(\\mathbf{x}) = (\\phi_1(\\mathbf{x}),\\ldots,\\phi_D(\\mathbf{x})).\n",
    "$$\n",
    "\n",
    "The collection of functions $\\phi_1(\\mathbf{x}), \\ldots, \\phi_D(\\mathbf{x})$ are the basis functions.  As we have done previously, we will use monomials up to a given degree as our basis functions.\n",
    "For example, for two dimensional data $\\mathbf{x} = (x_1, x_2)$ and monomials with degree up to 2 the resulting set of features is\n",
    "\n",
    "$$\n",
    "(1, x_1, x_2, x_1^2, x_2^2, x_1 x_2).\n",
    "$$\n",
    "\n",
    "The dot product in the space of our polynomial features takes on the form:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{\\top} \\Phi(\\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1^2 + w_4 x_2^2 + w_5 x_1 x_2\n",
    "$$\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb467d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(2, include_bias=False)\n",
    "# include bias determines if to include the case where all feature \n",
    "# powers are zero; this serves as a bias term \n",
    "poly.fit_transform(np.array([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa5974",
   "metadata": {},
   "source": [
    "Here's code for plotting the decision surface of the SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f361c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_str = ['or', '+b']\n",
    "xmin = -1\n",
    "xmax = 1\n",
    "ymin = -1\n",
    "ymax = 1\n",
    "\n",
    "def decision_surface(classifier, X, y, ax=None) :\n",
    "    is_svm = True\n",
    "    markersize=5\n",
    "    fontsize = 'medium'\n",
    "    contour_fontsize = 10\n",
    "    # setting up the grid\n",
    "    delta = 0.01\n",
    "    xx = np.arange(xmin, xmax, delta)\n",
    "    yy = np.arange(ymin, ymax, delta)\n",
    "\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.stack([XX.ravel(), YY.ravel()], axis=-1)\n",
    "    # alternatively:\n",
    "    #xy = np.column_stack([XX.ravel(), YY.ravel()])\n",
    "    #print(xy.shape)\n",
    "\n",
    "    Z = classifier.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    if ax is None :\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        ax = fig.add_subplot()\n",
    "    im = ax.imshow(np.transpose(Z), \n",
    "                   interpolation='bilinear', origin='lower',\n",
    "                   cmap=plt.cm.gray, extent=(xmin,xmax,ymin,ymax) )\n",
    "\n",
    "    if (is_svm) :\n",
    "        C = ax.contour(np.transpose(Z),\n",
    "                        [-1,0,1],\n",
    "                        origin='lower',\n",
    "                        linewidths=(1,3,1),\n",
    "                        colors = 'black',\n",
    "                        extent=(xmin,xmax,ymin,ymax))\n",
    "    else :\n",
    "        C = ax.contour(np.transpose(Z),\n",
    "                        [0],\n",
    "                        origin='lower',\n",
    "                        linewidths=(3),\n",
    "                        colors = 'black',\n",
    "                        extent=(xmin,xmax,ymin,ymax))\n",
    "\n",
    "    ax.clabel(C, inline=1, fmt='%1.1f',\n",
    "              fontsize=contour_fontsize)\n",
    "    # plot the data\n",
    "    for i in range(len(X)) :\n",
    "        ax.plot(X[i][0], X[i][1], plot_str[y[i]], markersize=5, alpha=0.5)\n",
    "    xticklabels = plt.getp(ax, 'xticklabels')\n",
    "    yticklabels = plt.getp(ax, 'yticklabels')\n",
    "    plt.setp(xticklabels, fontsize=fontsize)\n",
    "    plt.setp(yticklabels, fontsize=fontsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98ce80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings('ignore') \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm\n",
    "\n",
    "svc = make_pipeline(PolynomialFeatures(degree=2,include_bias=False), \n",
    "                     svm.SVC(kernel=\"linear\", C=10))\n",
    "\n",
    "svc.fit(X,y)\n",
    "decision_surface(svc, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0978f",
   "metadata": {},
   "source": [
    "Let's try to understand this a little better by looking at the data in the transformed space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf31f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_trans = poly.fit_transform(X)\n",
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ede65264",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "ax  = fig.add_subplot()\n",
    "colors = ['r' if y[i] == 0 else 'b' for i in range(len(y))]\n",
    "ax.scatter(X_trans[:, 2], X_trans[:, 4], c=colors, s=50, alpha=0.6) \n",
    "ax.set_xlabel(r'$x_1^2$', fontsize=14)\n",
    "ax.set_ylabel(r'$x_2^2$', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd5b5e",
   "metadata": {},
   "source": [
    "While the original data was clearly not linearly separable, in the transformed space it is very close to being so!\n",
    "What we have done, is transform the data in such a way that a linear classifier is all we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed809031",
   "metadata": {},
   "source": [
    "### Digression:  the soft margin constant and regularization\n",
    "\n",
    "Now, if we increase the value of $C$, the margin shrinks as we would expect, but the overall shape of the decision boundary is unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee95f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = make_pipeline(PolynomialFeatures(degree=2,include_bias=False), \n",
    "                     svm.SVC(kernel=\"linear\", C=1000))\n",
    "\n",
    "svc.fit(X,y)\n",
    "decision_surface(svc, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6fd640",
   "metadata": {},
   "source": [
    "Now let's increase the power of the monomials used to generate our features.  For a relatively low value of $C$ the shape of the boundary is still more or less a circle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799f3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc5 = make_pipeline(PolynomialFeatures(5), \n",
    "                     svm.SVC(kernel=\"linear\", C=10))\n",
    "\n",
    "svc5.fit(X,y)\n",
    "decision_surface(svc5, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ac661",
   "metadata": {},
   "source": [
    "However, when we increase the value of $C$, i.e. penalize misclassifications and margin violations, the classifier does its best to fit the data, even if that's not a good idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "218a12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc5 = make_pipeline(PolynomialFeatures(5), \n",
    "                     svm.SVC(kernel=\"linear\", C=1000))\n",
    "\n",
    "svc5.fit(X,y)\n",
    "decision_surface(svc5, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b69664",
   "metadata": {},
   "source": [
    "This highlights the role of the soft-margin constant $C$ in controling the extent of regularization.\n",
    "\n",
    "Recall that the soft-margin SVM is defined by the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{\\mathbf{w}, b}{\\mathrm{minimize~}}\n",
    " & \\frac{1}{2} ||\\mathbf{w}||^2  + C \\sum_{i=1}^N \\xi_i  \\\\\n",
    "\\mathrm{subject~to:}~~  & y_i \\left( \\mathbf{w}^{\\top} \\mathbf{x}_i + b \\right) \\geq 1 - \\xi_i ~~ i = 1,\\ldots,N \\,,\\\\\n",
    "& \\xi_i \\geq 0 ~~ i = 1,\\ldots,N \\,.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The first term, controls the margin and serves as a regularizer, while the second term controls how hard the classifier tries to classify every data point correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a9a5ab",
   "metadata": {},
   "source": [
    "### Mapping your data is not feasible for high dimensional data\n",
    "\n",
    "The approach of mapping data to a higher dimensional space using a nonlinear function can be highly effective, but suffers from a big drawback:  it becomes less and less feasible as the dimensionality of the input data increases.  SVMs and other kernel-based methods address this issue by *implicitly* mapping the data to a high dimensional space using *kernels*.  \n",
    "\n",
    "### Can we get the benefit of mapping our data without explicitly doing so?\n",
    "\n",
    "The answer to this question is yes.  To get an idea of how that might work, consider the function\n",
    "\n",
    "$$\n",
    "\\Phi(\\mathbf{x}) = (x_1^2, \\sqrt{2}x_1x_2, x_2^2)^{\\top}\n",
    "$$\n",
    "\n",
    "If we map the data using this function then the decision function of our linear classifier is non-linear in terms of our original features:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{\\top} \\Phi(\\mathbf{x}) = w_1x_1^2 + \\sqrt{2} w_2 x_1 x_2 + w_3 x_2^2\n",
    "$$\n",
    "\n",
    "So far we have not gained anything.  However, suppose the weight vector can be expressed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\sum_{i=1}^N \\alpha_i \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "In the space of the transformed data:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\sum_{i=1}^N \\alpha_i \\Phi(\\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The discriminant function is then:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^\\top \\Phi(\\mathbf{x}) + b = \\sum_{i=1}^N \\alpha_i \\Phi(\\mathbf{x}_i)^\\top \\Phi(\\mathbf{x}) + b\n",
    "$$\n",
    "\n",
    "What we see is that it depends on our data only through dot products, and it turns out that for an appropriately chosen function $\\Phi$, we can compute the dot product without explicitly mapping the data into a high dimensional feature space.  For example, let's consider our mapping from above \n",
    "\n",
    "$$\n",
    "\\Phi(\\mathbf{x}) = (x_1^2, \\sqrt{2}x_1x_2, x_2^2)^{\\top}\n",
    "$$\n",
    "\n",
    "And compute \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Phi(\\mathbf{x})^\\top \\Phi(\\mathbf{z}) & = (x_1^2, \\sqrt{2}x_1x_2, x_2^2)^{\\top} (z_1^2, \\sqrt{2}z_1 z_2, z_2^2) \\\\\n",
    "     & = x_1^2 z_1^2 + 2 x_1 x_2 z_1 z_2 + x_2^2 z_2^2 \\\\\n",
    "     & = (\\mathbf{x}^\\top \\mathbf{z})^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This means, that if we can express our machine learning algorithm using dot products, we don't need to compute the mapping explicitly:\n",
    "Squaring the dot product in the original space has the same effect as computing the dot product in the transformed space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6b973",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "**Definition:**  A function $k(\\mathbf{x}, \\mathbf{z})$ that can be expressed as a dot product in some feature space is called a kernel.\n",
    "\n",
    "In other words, $k(\\mathbf{x}, \\mathbf{z})$ is a kernel if there exists a function $\\Phi: \\mathcal{x} \\mapsto \\mathcal{F}$\n",
    "such that \n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{z}) = \\Phi(\\mathbf{x})^\\top \\Phi(\\mathbf{z})\n",
    "$$\n",
    "\n",
    "Why is this interesting?\n",
    "\n",
    "If the algorithm can be expressed in terms of dot products, we can work in the feature space without performing the mapping explicitly!\n",
    "And it turns out the large margin optimization problem used to train SVMs can be expressed in terms of dot products.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb12b9",
   "metadata": {},
   "source": [
    "### Standard kernel functions\n",
    "\n",
    "The linear kernel\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x}^\\top \\mathbf{z}\n",
    "$$\n",
    "\n",
    "The homogeneous polynomial kernel of degree $D$\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z})^D\n",
    "$$\n",
    "\n",
    "The polynomial kernel of degree $D$\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{z}) = (1 + \\mathbf{x}^\\top \\mathbf{z})^D\n",
    "$$\n",
    "\n",
    "The Gaussian kernel\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\gamma ||\\mathbf{x} - \\mathbf{z}||^2)\n",
    "$$\n",
    "\n",
    "The Gaussian kernel is named after its similarity to the expression for the Gaussian distribution function (aka normal).  More on that below.\n",
    "\n",
    "Each of these kernels implicitly represents the data in a high dimensional feature space.  Do you have some ideas on what those might be?  For the Gaussian kernel, it happens to be infinite dimensional.  This is discussed in detail e.g. in:\n",
    "\n",
    "* Shawe-Taylor, John, and Nello Cristianini. Kernel methods for pattern analysis. Cambridge university press, 2004.\n",
    "\n",
    "### Visualizing the SVM decision boundary for different kernels and their parameters\n",
    "\n",
    "In the following code cells we will take a look at what the SVM is doing for different kernels and different kernel parameters.\n",
    "\n",
    "First some toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b64aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-0.7289242898264241, -0.13617135002499547],\n",
    " [-0.6668275156328757, -0.27902849288213827],\n",
    " [-0.7067468704715854, -0.41593325478690024],\n",
    " [-0.5470694511167467, -0.6004570643107098],\n",
    " [-0.39182751563287566, -0.4278380166916621],\n",
    " [-0.3696500962780369, -0.29093325478690013],\n",
    " [-0.2676339672457788, -0.04093325478690035],\n",
    " [-0.44948880595545626, -0.0349808738345192],\n",
    " [-0.4627952575683595, 0.179304840451195],\n",
    " [-0.18335977369739176, -0.320695159548805],\n",
    " [-0.17448880595545635, -0.7135523024059478],\n",
    " [-0.40513396724577877, -0.8802189690726145],\n",
    " [-0.3252952575683594, -0.6183142071678526],\n",
    " [-0.05473074143932721, -0.0885523024059478],\n",
    " [-0.14344041885868197, 0.16740007854643313],\n",
    " [-0.5825533220844885, 0.8102572214035757],\n",
    " [-0.24989203176190777, 0.8400191261654808],\n",
    " [0.07389829081873733, 0.8400191261654808],\n",
    " [0.33559183920583413, 0.7685905547369092],\n",
    " [0.544059581141318, 0.5900191261654808],\n",
    " [0.6549466779155115, 0.40549531664167127],\n",
    " [0.7170434521090598, 0.14359055473690918],\n",
    " [0.7392208714638988, -0.005218969072614543],\n",
    " [0.7613982908187373, -0.15402849288213827],\n",
    " [0.7791402263026082, -0.4397427785964241],\n",
    " [0.7835757101735763, -0.7195046833583288],\n",
    " [0.765833774689705, -0.8147427785964241],\n",
    " [0.4109950650122858, 0.06620960235595685],\n",
    " [0.4376079682380922, 0.5840667452130996],\n",
    " [0.7214789359800275, 0.7983524594988138],\n",
    " [0.9255111940445437, 0.5840667452130996],\n",
    " [0.4109950650122858, 0.8876381737845283],\n",
    " [0.029543452109059842, 0.9947810309273855],\n",
    " [-0.6535210640199725, 0.9590667452130999],\n",
    " [-0.0414242898264241, 0.6674000785464329],\n",
    " [0.2113982908187373, 0.5364476975940522],\n",
    " [0.6859950650122859, 0.655495316641671],\n",
    " [0.526317645657447, -0.27307611192975734],\n",
    " [0.6948660327542213, -0.6123618262154716],\n",
    " [0.6460757101735761, 0.0364476975940522],\n",
    " [0.5618015166251888, 0.3281143642607187],\n",
    " [-0.5958597736973917, -0.005218969072614543],\n",
    " [-0.7954565478909402, -0.011171350024995474],\n",
    " [-0.7555371930522305, -0.6242665881202336],\n",
    " [-0.8131984833748112, -0.5528380166916621],\n",
    " [-0.6978759027296498, 0.2507334118797664],\n",
    " [-0.7998920317619079, 0.8995429356892901],\n",
    " [0.5618015166251888, -0.7433142071678526]])\n",
    "y = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1])\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax  = fig.add_subplot(111)\n",
    "colors = ['r' if y[i] == 0 else 'b' for i in range(len(y))]\n",
    "ax.scatter(X[:, 0], X[:, 1], c=colors, s=50, alpha=0.6) \n",
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366044f",
   "metadata": {},
   "source": [
    "Let's start with the polynomial kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba21bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly = svm.SVC(kernel=\"poly\", degree=2, coef0=1, C=10)\n",
    "\n",
    "svm_poly.fit(X,y)\n",
    "decision_surface(svm_poly, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429f87a",
   "metadata": {},
   "source": [
    "Next, the Gaussian kernel (note that it is called 'rbf' in scikit-learn, which is short for radial basis function which is in reference to radial basis function neural networks, which used this kernel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66a2b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf = svm.SVC(kernel=\"rbf\", gamma=1, C=10)\n",
    "\n",
    "svm_rbf.fit(X,y)\n",
    "decision_surface(svm_rbf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ab5f8",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Explore the toy dataset above by varying the value of the parameter $\\gamma$ of the Gaussian kernel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940863e",
   "metadata": {},
   "source": [
    "In order to understand the behavior of the Gaussian kernel, let's take a look at the Gaussian distribution as a function of its width parameter:\n",
    "\n",
    "Recall that the Gaussian kernel is defined as:\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\gamma ||\\mathbf{x} - \\mathbf{z}||^2)\n",
    "$$\n",
    "\n",
    "And compare that to the expression for the Gaussian distribution:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(- \\left(\\frac{x-\\mu}{\\sigma} \\right)^2 \\right),\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the standard deviation and $\\mu$ is the mean.  The parameter $\\sigma$ controls the width of the distribution.\n",
    "To understand the effect of $\\gamma$, which is inversely proportional to the width parameter of the Gaussian distribution let's plot Gaussian distributions with different values of the width parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcb5142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "gamma1 = 1\n",
    "sigma1 = 1/np.sqrt(gamma1)\n",
    "gamma2 = 20\n",
    "sigma2 = 1/np.sqrt(gamma2)\n",
    "\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(42) \n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "x1 = rng.normal(scale=sigma1, size=(sample_size,)) \n",
    "x2 = rng.normal(scale=sigma2, size=(sample_size,)) \n",
    "\n",
    "num_bins = 25\n",
    "plt.hist(x1, num_bins, density=True, facecolor='green', alpha=0.4, \n",
    "         edgecolor = 'black', label='gamma=' + str(gamma1))\n",
    "plt.hist(x2, num_bins, density=True, facecolor='blue', alpha=0.4, \n",
    "         edgecolor = 'black', label='gamma=' + str(gamma2))\n",
    "plt.xlabel('x',fontsize=12)\n",
    "plt.ylabel('normalized counts', fontsize=12)\n",
    "\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d440b1",
   "metadata": {},
   "source": [
    "### Nonlinear SVM on real data\n",
    "\n",
    "A question frequently posed by practitioners is *which kernel\n",
    "should I use for my data?* There are several answers to this question. The first is that it is, like most practical questions in machine\n",
    "learning, data dependent, so several kernels should be tried. \n",
    "That being said, we typically follow the following procedure: try a linear kernel first, and then see if you can improve on its performance\n",
    "using a nonlinear kernel. The linear kernel provides a useful baseline,\n",
    "and in many applications provides the best results.\n",
    "The flexibility of the Gaussian and polynomial kernels can lead to\n",
    "overfitting in high-dimensional datasets. Furthermore,\n",
    "an SVM with a linear kernel is easier to tune since the only parameter\n",
    "that affects performance is the soft-margin constant. Once a result\n",
    "using a linear kernel is available, it can serve as a baseline that you can try to improve upon using a nonlinear kernel. Between the Gaussian\n",
    "and polynomial kernels, our experience shows that the Gaussian\n",
    "kernel usually outperforms the polynomial kernel in both accuracy\n",
    "and convergence time if the data are normalized correctly and a\n",
    "good value of the width parameter and soft-margin constant are chosen.\n",
    "\n",
    "Let's explore the breast cancer dataset with various kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d221c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X,y = data = load_breast_cancer(return_X_y = True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "393feda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0d429",
   "metadata": {},
   "source": [
    "Let's try the nonlinear SVM with a Gaussian kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "585aee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(kernel=\"rbf\", gamma=1, C=10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "print('accuracy: ', np.mean(y_test == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbdd24",
   "metadata": {},
   "source": [
    "This was a complete failure! The Gaussian kernel requires careful tuning of the width hyper-parameter $\\gamma$.  Let's see how performance varies as we vary its value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "990c7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = np.logspace(-6, 2, num=9, endpoint=True, base=10.0)\n",
    "\n",
    "accuracies = []\n",
    "training_accuracies = []\n",
    "for gamma in gammas :\n",
    "    classifier = svm.SVC(kernel=\"rbf\", gamma=gamma, C=10)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = np.mean(y_test == y_pred)\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    training_accuracy = np.mean(y_train==y_pred_train)\n",
    "    training_accuracies.append(training_accuracy)\n",
    "    print(f\"gamma: {gamma}\\t accuracy: {accuracy:0.3f}\")\n",
    "    accuracies.append(np.mean(y_test == y_pred))\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(5,4))\n",
    "plt.semilogx(gammas, accuracies, 'ob', alpha=0.7, \n",
    "             label=\"test set accuracy\");\n",
    "plt.semilogx(gammas, training_accuracies, 'or', alpha=0.7, \n",
    "             label=\"training set accuracy\");\n",
    "plt.xlabel(r'$\\gamma$', fontsize=16)\n",
    "plt.ylabel(\"accuracy\", fontsize=16)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7d4e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline \n",
    "    \n",
    "gammas = np.logspace(-6, 2, num=9, endpoint=True, base=10.0)\n",
    "\n",
    "accuracies = []\n",
    "training_accuracies = []\n",
    "for gamma in gammas :\n",
    "    classifier = Pipeline([('scaler', StandardScaler()), \n",
    "                           ('svc', svm.SVC(kernel=\"rbf\", gamma=gamma, C=10))])\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = np.mean(y_test == y_pred)\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    training_accuracy = np.mean(y_train==y_pred_train)\n",
    "    training_accuracies.append(training_accuracy)\n",
    "    print(f\"gamma: {gamma}\\t accuracy: {accuracy:0.3f}\")\n",
    "    accuracies.append(np.mean(y_test == y_pred))\n",
    "    \n",
    "plt.figure(figsize=(5,4))\n",
    "plt.semilogx(gammas, accuracies, 'ob', alpha=0.7, \n",
    "             label=\"validation accuracy\");\n",
    "plt.semilogx(gammas, training_accuracies, 'or', alpha=0.7, \n",
    "             label=\"training accuracy\");\n",
    "plt.xlabel(r'$\\gamma$', fontsize=16)\n",
    "plt.ylabel(\"accuracy\", fontsize=16)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c94c75",
   "metadata": {},
   "source": [
    "As an aside, let's run the SVM with Gaussian kernel with the default value of $\\gamma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52b9f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(kernel=\"rbf\")\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "print('accuracy: ', np.mean(y_test == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7474db",
   "metadata": {},
   "source": [
    "The scikit-learn SVC implementation uses a clever way to set the value of $\\gamma$ as described in the [SVC documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html); however, while this is a reasonable choice, it's usually not the optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd49627",
   "metadata": {},
   "source": [
    "As yet another way to preprocess the data, we will convert our feature vectors into unit vectors, and see how that affects the performance of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "380a70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = X.copy()\n",
    "X_normalized = StandardScaler().fit_transform(X_normalized)\n",
    "for i in range(len(X_normalized)): \n",
    "    X_normalized[i] = X_normalized[i] / np.linalg.norm(X_normalized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "339fdab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(X_normalized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6e9574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized, X_test_normalized, y_train, y_test = train_test_split(\n",
    "    X_normalized, y, test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54fbf37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = np.logspace(-6, 2, num=9, endpoint=True, base=10.0)\n",
    "\n",
    "accuracies = []\n",
    "for gamma in gammas :\n",
    "    classifier = svm.SVC(kernel=\"rbf\", gamma=gamma, C=10)\n",
    "    classifier.fit(X_train_normalized, y_train)\n",
    "    y_pred = classifier.predict(X_test_normalized)\n",
    "    accuracy = np.mean(y_test == y_pred)\n",
    "    print(f\"gamma: {gamma}\\t accuracy: {accuracy:0.3f}\")\n",
    "    accuracies.append(np.mean(y_test == y_pred))\n",
    "    \n",
    "plt.figure(figsize=(5,4))\n",
    "plt.semilogx(gammas, accuracies, 'ob', alpha=0.7);\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04552f",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "In this notebook we explored how to convert a linear classifier into a non-linear classifier by replacing dot products with kernel functions.  This is a very powerful approach that was very dominant in the machine learning community.  Problem solving using kernels is about designing kernels that are appropriate for representing a given domain.  For approaches relevant for DNA and RNA sequences for example, see:\n",
    "\n",
    "* Ben-Hur, Asa, Cheng Soon Ong, Sören Sonnenburg, Bernhard Schölkopf, and Gunnar Rätsch. [Support vector machines and kernels for computational biology](http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1000173). PLoS computational biology 4, no. 10 (2008): e1000173.\n",
    "\n",
    "An entirely different approach to achieving non-linearity is to use neural networks.  Neural networks produce non-linear decision boundaries by using multiple layers of perceptron-like units.  Whereas the non-linearity provided by kernels is fixed by the choice of the kernel, neural networks essentially learn a nonlinearity that is appropriate for representing the given classification problem as we will see later in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
